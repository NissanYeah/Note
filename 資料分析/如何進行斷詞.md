# 如何用Python和jieba進行斷詞？

## 安裝套件jieba

[jieba](https://github.com/fxsjy/jieba)是一個方便「中文斷詞」的模型：
- 對於已收錄的字詞：透過餵食模型語料，它會自己找出字詞組合中機率最大的路徑
- 對於未收錄的字詞：透過觀察可能的序列

jieba支援以下功能
- 斷詞：例如把「台北是一個城市」，斷成「台北/是/一個/城市」
- 詞性標注：例如「台北/名詞」
- 可以自訂字典、停用字：把未收錄的字詞加進去，例如「R語言」。把不關心的字詞過濾掉，例如「的」

```
pip3 install jieba
```


## 使用範例

1. 對於「台北要去哪裡買衣服啊」進行斷詞

```python
import jieba
for ele in jieba.cut('台北要去哪裡買衣服啊'):
    print(ele)
```

會得到以下結果

```
台北
要
去
哪裡
買
衣服
啊
```

2. 對於擁多行的csv檔案進行斷詞（以dcard的購物版為例）

```py
import jieba
import csv
file = 'title.csv'  #csv檔案路徑

#開啟檔案並且切詞
with open(file, 'r') as file:
     segments += jieba.cut(row)

#印出切詞
for ele in segments:
    print(ele)
```

會得到以下結果

```
title
#
淘金
幣
1
/
1
組隊
領金幣
辣妹子
的
價差
我
不能
接受
```




## 過濾掉停止詞彙

```py
#進行切詞，並且設定停止詞彙
import jieba
import csv
file = 'title.csv'  #csv檔案路徑
file_stop = 'stop_words.txt'  #停止詞彙

stopWords = []

# 拿出停止詞彙
with open(file_stop, 'r') as file:
     for row in file.readlines():
         row = row.strip()
         stopWords.append(row)

# 進行切詞
with open(file_stop, 'r') as file:
     segments += jieba.cut(row)

# 過濾掉停止詞彙
remainderWords = list(filter(lambda a: a not in stopWords and a != '\n', segments))

for k in remainderWords:
    print(k)

```